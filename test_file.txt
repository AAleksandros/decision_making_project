Open In Colab
Work in Decision Making Systems

Groups of 3 people deadline: 18/6/2023

In this paper we will manage and analyze open and synthetic data to draw conclusions, make recommendations and make decisions about the selection of music bands, as well as the purchase/sale of records. To extract the opendata we will use the Last.fm Music Discovery API [1] and/or the discogs API [2]

[1] https://www.last.fm/api

[2] https://www.discogs.com/developers


Required modules installation.


!pip install mysql.connector
!pip install names
!pip install statsmodels
!pip install pmdarima
!pip install deap

from datetime import datetime
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from deap import creator, base, tools, algorithms
from math import sqrt
import networkx as nx
import numpy as np
import pandas as pd
import json
import math
import random
import names
import requests
import time
import socket
import seaborn as sns
import mysql.connector
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/gdrive')
     
Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount("/content/gdrive", force_remount=True).
Configuration file for secure storage of sensitive information.


# Read and retrieve the data from the configuration file.
with open('/content/gdrive/My Drive/config_data.json', 'r') as f:
    config_data = json.load(f)

db_host = config_data['DB_HOST']
db_port = config_data['DB_PORT']
db_user = config_data['DB_USER']
db_password = config_data['DB_PASSWORD']
db_database = config_data['DB_DATABASE']
lastfm_api_key = config_data['LASTFM_API_KEY']
discogs_api_token = config_data['DISCOGS_API_TOKEN']
     
Connectivity test from google colab to mydb in docker desktop mariadb container


try:
    # Create a socket object.
    sock = socket.create_connection((db_host, db_port), timeout=5)

    # If the connection succeeds, print a success message.
    print(f"Successfully connected to {db_host}:{db_port}")

    # Close the socket connection.
    sock.close()

except Exception as e:
    # If there is an error, print the error message.
    print(f"Error: {e}")
     
Successfully connected to 46.246.177.50:3306
Queries to create the required relational database schema.


mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)
cursor = mydb.cursor()

cursor.execute("CREATE TABLE Users (user_id INT(11) NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255));")
cursor.execute("CREATE TABLE Bands (band_id INT(11) NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), genre VARCHAR(255), playcount INT(11), listeners INT(11), entry_timestamp TIMESTAMP);")
cursor.execute("CREATE TABLE Records (record_id INT(11) NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255), band_id INT(11), price FLOAT, FOREIGN KEY (band_id) REFERENCES Bands(band_id));")
cursor.execute("CREATE TABLE User_Bands (user_id INT(11) NOT NULL, band_id INT(11) NOT NULL, PRIMARY KEY (user_id, band_id), FOREIGN KEY (user_id) REFERENCES Users(user_id), FOREIGN KEY (band_id) REFERENCES Bands(band_id));")
cursor.execute("CREATE TABLE User_Records (user_id INT(11) NOT NULL, record_id INT(11) NOT NULL, PRIMARY KEY (user_id, record_id), FOREIGN KEY (user_id) REFERENCES Users(user_id), FOREIGN KEY (record_id) REFERENCES Records(record_id));")

mydb.commit()
cursor.close()
mydb.close()
     
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-aec936fb3061> in <cell line: 1>()
----> 1 mydb = mysql.connector.connect(
      2     host=db_host,
      3     port=db_port,
      4     user=db_user,
      5     password=db_password,

NameError: name 'mysql' is not defined

Suppose you have 20 users forming a community as given by a Barabasi Albert model [3].

community_graph= nx.barabasi_albert_graph(20,3)
nx.draw(community_graph, with_labels=True)

     

(a.) Design the data model that will represent the users of the community, the bands they like and the records they own. Implement the data model scheme in a relational (SQL like) database (DB) and build functions that insert random data of your choice for users. [Lecture 3 MySQL]

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

# Define a function to insert a user.
def insert_user(name):
    add_user = ("INSERT INTO Users (name) VALUES (%s)")
    cursor.execute(add_user, (name,))

# Check the number of users in the table and then, if the user count is already 20, print a message and skip the insertion.
cursor.execute("SELECT COUNT(*) FROM Users")
user_count = cursor.fetchone()[0]

if user_count >= 20:
    print("User table is already full.")
else:
    for _ in range(20):
        insert_user(names.get_full_name())
    mydb.commit()

mydb.close()
     
User table is already full.

(b.) For each user of the community, create functions that retrieve data from opendata in relation to the bands he listens to and the records he is interested in and to fill the corresponding tables in the NW. [Rest calls in [1] [2] and MySQL Lecture 3]

Note: For the selection of attributes, consider the following questions. It is natural as you become familiar with the task to change the shape of the NW and the attributes).

[1] https://www.last.fm/api

[2] https://www.discogs.com/developers


mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

# Get the top artists from the Last.fm API.
response = requests.get('http://ws.audioscrobbler.com/2.0/?method=chart.gettopartists&api_key=' + lastfm_api_key + '&format=json')
top_artists = response.json()['artists']['artist']
# Wait for 1 second to avoid hitting the API rate limit.
time.sleep(1)

# Get the user IDs from the database.
cursor.execute("SELECT user_id FROM Users")
user_ids = [row[0] for row in cursor.fetchall()]

# Define the order of record quality preference.
price_preference_order = ['Very Good Plus (VG+)', 'Near Mint (NM or M-)', 'Mint (M)', 'Very Good (VG)', 'Good Plus (G+)', 'Good (G)', 'Fair (F)', 'Poor (P)']

for user_id in user_ids:
    # Check if the user already has at least 2 artists linked to them.
    cursor.execute("SELECT COUNT(*) FROM User_Bands WHERE user_id = %s", (user_id,))
    count = cursor.fetchone()[0]
    if count >= 2:
        print("User " + str(user_id) + " already has at least 2 artists linked to them. Skipping user...")
        continue
    # Randomly assign 2 to 5 artists to the user.
    liked_artists = random.sample(top_artists, random.randint(2, 5))
    for artist in liked_artists:
        # Check if the artist is already in the Bands table.
        cursor.execute("SELECT band_id FROM Bands WHERE name = %s", (artist['name'],))
        result = cursor.fetchone()
        if result is None:
            # Get genre information for the artist, insert the artist into the Bands table and then get the ID of the newly inserted band.
            response = requests.get('http://ws.audioscrobbler.com/2.0/?method=artist.getinfo&artist=' + artist['name'] + '&api_key=' + lastfm_api_key + '&format=json')
            artist_info = response.json()
            genre = artist_info['artist']['tags']['tag'][0]['name'] if artist_info['artist']['tags']['tag'] else None
            cursor.execute("INSERT INTO Bands (name, genre) VALUES (%s, %s)", (artist['name'], genre))
            mydb.commit()
            band_id = cursor.lastrowid
            time.sleep(1)
        else:
            print("Artist or band " + artist['name'] + " with id " + str(band_id) + " is already present in the Bands table. Skipping artist/band...")
            band_id = result[0]

        # Link the user and the band in the User_Bands table if not already linked.
        cursor.execute("SELECT * FROM User_Bands WHERE user_id = %s AND band_id = %s", (user_id, band_id))
        if cursor.fetchone() is None:
            cursor.execute("INSERT INTO User_Bands (user_id, band_id) VALUES (%s, %s)", (user_id, band_id))
            mydb.commit()
        else:
           print("User " + str(user_id) + " and artist/band " + artist['name'] + " with id " + str(band_id) + " are already linked. Skipping link creation...")


        # Use the Discogs API to get the albums of the artist and then create a list to use later in album assignment.
        response = requests.get('https://api.discogs.com/database/search?type=release&artist=' + artist['name'] + '&token=' + discogs_api_token)
        albums = response.json()['results']
        time.sleep(1)
        owned_albums = random.sample(albums, random.randint(1, min(5, len(albums))))

        for album in owned_albums:
            # Check if the user already has at least one record linked to them that belongs to the current artist.
            cursor.execute("SELECT COUNT(*) FROM User_Records JOIN Records ON User_Records.record_id = Records.record_id WHERE User_Records.user_id = %s AND Records.band_id = %s", (user_id, band_id))
            count = cursor.fetchone()[0]
            if count > 0:
                print("User " + str(user_id) + " already has at least one record linked to them belonging to current artist/band " + str(band_id) + ". Skipping link creation...")
                continue

            # Check if the album is already in the Records table.
            cursor.execute("SELECT record_id FROM Records WHERE name = %s AND band_id = %s", (album['title'], band_id))
            result = cursor.fetchone()
            if result is None:
                # Get the price suggestions for the album from the Discogs API and then iterate over the conditions in the order of preference.
                response = requests.get('https://api.discogs.com/marketplace/price_suggestions/' + str(album['id']), headers={'Authorization': 'Discogs token=' + discogs_api_token})
                price_suggestions = response.json()
                price = None

                for condition in price_preference_order:
                    # If the condition is in the price suggestions, get the price and break the loop.
                    if condition in price_suggestions:
                        price = price_suggestions[condition]['value']
                        break

                # Insert the album into the Records table and get the ID of the newly inserted record.
                cursor.execute("INSERT INTO Records (name, band_id, price) VALUES (%s, %s, %s)", (album['title'], band_id, price))
                mydb.commit()
                record_id = cursor.lastrowid
                time.sleep(1)
            else:
                print(" Record " + album['title'] + " already present in records table. Skipping insertion...")
                record_id = result[0]

            # Link the user and the album in the User_Records table if not already linked.
            cursor.execute("SELECT * FROM User_Records WHERE user_id = %s AND record_id = %s", (user_id, record_id))
            if cursor.fetchone() is None:
                cursor.execute("INSERT INTO User_Records (user_id, record_id) VALUES (%s, %s)", (user_id, record_id))
                mydb.commit()
            else:
              print("User " + str(user_id) + " and record " + str(record_id) + " already linked. Skipping link creation...")
     

(c.) Implement a function that checks for and deals with [missing](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b) [4], duplicate [5] or outlier [6] values in any of the NW attributes.

[4] https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b

[5] https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python

[6] https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

# Calculate the average price of all records and get all records with NULL price.
cursor.execute("SELECT AVG(price) FROM Records WHERE price IS NOT NULL")
average_price = cursor.fetchone()[0]
cursor.execute("SELECT record_id FROM Records WHERE price IS NULL")
records = cursor.fetchall()

null_prices_updated = 0

for record in records:
    record_id = record[0]

    # Generate a random percentage between -25% and +25%, calculate the new price and then update the price in the Records table.
    percentage = random.uniform(-0.25, 0.25)
    new_price = average_price * (1 + percentage)
    cursor.execute("UPDATE Records SET price = %s WHERE record_id = %s", (new_price, record_id))
    mydb.commit()
    null_prices_updated += 1

print("Number of null prices updated:", null_prices_updated)
     
(d.) Draw some statistical conclusions about the data features [7] [8]

[7] https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html

[8] https://sparkbyexamples.com/pandas/calculate-summary-statistics-in-pandas/


mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

# Execute the SQL query
cursor.execute("""
    SELECT Bands.genre, COUNT(DISTINCT User_Bands.user_id) as user_count
    FROM User_Bands
    JOIN Bands ON User_Bands.band_id = Bands.band_id
    GROUP BY Bands.genre
    ORDER BY user_count DESC
""")

rows1 = cursor.fetchall()
for row in rows1:
    print("Genre: ", row[0], ", Number of Users: ", row[1])
     

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

# Fetch the data from the database, convert rows2 to a dataframe,  drop duplicates from the dataframe and create a matrix where each cell represents the number of users who like both genres.
cursor.execute("""
    SELECT Users.user_id, Bands.genre
    FROM Users
    JOIN User_Bands ON Users.user_id = User_Bands.user_id
    JOIN Bands ON User_Bands.band_id = Bands.band_id
""")
rows2 = cursor.fetchall()

df = pd.DataFrame(rows2, columns=['user_id', 'genre'])
df = df.drop_duplicates()

user_genre_matrix = df.pivot_table(index='user_id', columns='genre', aggfunc='size', fill_value=0)
co_occurrence_matrix = user_genre_matrix.T.dot(user_genre_matrix)

print(co_occurrence_matrix)
     

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

#Adjust the Records table to accept new data about playcount, listeners of each record and also provide a timestamp.

cursor.execute("ALTER TABLE Bands ADD COLUMN playcount INT")
cursor.execute("ALTER TABLE Bands ADD COLUMN listeners INT")
cursor.execute("ALTER TABLE Bands ADD COLUMN entry_timestamp TIMESTAMP")
mydb.commit()
     
Then we will fetch this new data for each band and store it in the Bands table.

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

cursor.execute("SELECT band_id, name FROM Bands")
bands = cursor.fetchall()

for band in bands:
    response = requests.get('http://ws.audioscrobbler.com/2.0/?method=artist.getinfo&artist=' + band[1] + '&api_key=' + lastfm_api_key + '&format=json')
    artist_info = response.json()
    playcount = artist_info['artist']['stats']['playcount']
    listeners = artist_info['artist']['stats']['listeners']
    time.sleep(1)

    cursor.execute("UPDATE Bands SET playcount = %s, listeners = %s, entry_timestamp = NOW() WHERE band_id = %s", (playcount, listeners, band[0]))
    mydb.commit()
     

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

cursor.execute("SELECT genre FROM Bands GROUP BY genre")
genres = cursor.fetchall()

for genre in genres:
    cursor.execute("SELECT SUM(playcount), SUM(listeners) FROM Bands WHERE genre = %s", (genre[0],))
    sums = cursor.fetchone()
    if sums[1] != 0:
        ratio = sums[0] / sums[1]
    else:
        ratio = 0
    print("Genre: " + genre[0] + ", Total Playcount: " + str(sums[0]) + ", Total Listeners: " + str(sums[1]) + ", Playcount-to-Listeners Ratio: " + str(ratio))
     

(e.) Graphically represent some attributes of your data and explain the conclusions you draw [data visualization lecture].

Representation of popularity of music categories among our users, using a bar plot.

# Split the rows into two lists: one for the genres and one for the user counts.
genres = [row[0] for row in rows1]
user_counts = [row[1] for row in rows1]

# Create a bar plot.
plt.barh(genres, user_counts)
plt.xlabel('Number of Users')
plt.ylabel('Genre')
plt.title('Popularity of Genres Among Users')
plt.gca().invert_yaxis()
plt.show()
     

Representation of overlap of music preferences of users by pairs, using a heatmap.

# Convert the co_occurrence_matrix to a floating-point type, set the diagonal to NaN and then create a heatmap.
co_occurrence_matrix = co_occurrence_matrix.astype(float)
np.fill_diagonal(co_occurrence_matrix.values, np.nan)
sns.heatmap(co_occurrence_matrix, cmap='YlGnBu')
plt.show()
     
In addition, a table with the above heatmap data.

# Set the diagonal values of co_occurrence_matrix to 0 and reset the index of the co-occurrence matrix to convert the index into columns.
np.fill_diagonal(co_occurrence_matrix.values, 0)
co_occurrence_matrix_reset = co_occurrence_matrix.reset_index()

# Melt the DataFrame to create a long format of DataFrame, which consists of pairs of genres and create a new column 'genre_set' which is a set of the two genres.
co_occurrence_pairs = co_occurrence_matrix_reset.melt(id_vars='genre', var_name='genre_2', value_name='overlap')
co_occurrence_pairs['genre_set'] = co_occurrence_pairs.apply(lambda x: frozenset([x['genre'], x['genre_2']]), axis=1)

# Drop duplicates based on 'genre_set' and sort the pairs by overlap in descending order.
co_occurrence_pairs = co_occurrence_pairs.drop_duplicates(subset='genre_set')
co_occurrence_pairs = co_occurrence_pairs.sort_values(by='overlap', ascending=False)

print(co_occurrence_pairs.head())

plt.figure(figsize=(10,6))
plt.bar(genres, playcounts, color='b', alpha=0.5, label='Playcounts')
plt.bar(genres, listeners, color='r', alpha=0.5, label='Listeners')
plt.xlabel('Genres')
plt.ylabel('Counts')
plt.title('Playcounts and Listeners by Genre')
plt.xticks(rotation=90)
plt.legend()
plt.show()
     


plt.figure(figsize=(15,10))
plt.scatter(playcounts, listeners)
for i, txt in enumerate(genres):
    plt.annotate(txt, (playcounts[i], listeners[i]), textcoords="offset points", xytext=(10,-10), ha='center')
plt.xlabel('Playcounts')
plt.ylabel('Listeners')
plt.title('Playcounts vs Listeners by Genre')
plt.show()
     
(f.) Let the synthetic time series dataset in the next cell have the lowest price that a disc is sold for each day, represent the time series and decompose it into trend, seasonality and residuals [lecture and lab time series].


date_rng = pd.date_range(start='1/1/2018', end='12/31/2022', freq='D')
val=40+15*np.tile(np.sin(np.linspace(-np.pi, np.pi, 365)),5)
val=np.append(val,val[1824])+5*np.random.rand(1826)
series = pd.DataFrame({
    'values': val
}, index=pd.DatetimeIndex(date_rng))
series.plot()
series.to_csv('gdrive/My Drive/File_series.csv', index=True, header=True)
     


# Decompose the time series
result = seasonal_decompose(series['values'], model='additive')

fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(10,10))
result.observed.plot(ax=ax1)
ax1.set_ylabel('Observed')
result.trend.plot(ax=ax2)
ax2.set_ylabel('Trend')
result.seasonal.plot(ax=ax3)
ax3.set_ylabel('Seasonal')
result.resid.plot(ax=ax4)
ax4.set_ylabel('Residual')

plt.tight_layout()
plt.show()
     
(g.) Split the time series data into 76% training and 34% testing, train an ARIMA model in the training split to predict the lowest price a record will sell for the next day, and evaluate its performance in the testing split [lecture and lab time series].


X = series.values
size = int(len(X) * 0.76)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
predictions = list()
     

plot_acf(series, lags=100)
plt.show()
     


plot_pacf(series, lags=10)
plt.show()
     
/usr/local/lib/python3.10/dist-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.
  warnings.warn(


for t in range(len(test)):
	model = ARIMA(history, order=(5,1,0))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	predictions.append(yhat)
	obs = test[t]
	history.append(obs)
	print('predicted=%f, expected=%f' % (yhat, obs))

rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
plt.plot(test)
plt.plot(predictions, color='red')
plt.show()
     
Test RMSE: 1.707


def evaluate_arima_model(X, arima_order):
    # Prepare training dataset.
    train_size = int(len(X) * 0.76)
    train, test = X[0:train_size], X[train_size:]
    history = [x for x in train]
    # Make predictions.
    predictions = list()
    for t in range(len(test)):
        model = ARIMA(history, order=arima_order)
        model_fit = model.fit()
        yhat = model_fit.forecast()[0]
        predictions.append(yhat)
        history.append(test[t])
    # Calculate out of sample error.
    error = mean_squared_error(test, predictions)
    print(error)

arima_order = (5, 1, 0)
error = evaluate_arima_model(X, arima_order)
     
2.7845327136507927

def evaluate_models(dataset, p_values, d_values, q_values):
    dataset = dataset.astype('float64')
    best_score, best_cfg = float("inf"), None
    for p in p_values:
        for d in d_values:
            for q in q_values:
                order = (p,d,q)
                try:
                    mse = evaluate_arima_model(dataset, order)
                    if mse < best_score:
                        best_score, best_cfg = mse, order
                    print('ARIMA%s MSE=%.3f' % (order,mse))
                except:
                    continue
    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))

p_values = [0, 1, 2, 3, 4, 5]
d_values = [0, 1, 2]
q_values = [0, 1, 2, 3, 4, 5]
evaluate_models(series.values, p_values, d_values, q_values)
     

(h.) Using some graph mining metric on the community graph to suggest to a user a disk that he has not selected [graph mining lecture].

mydb = mysql.connector.connect(
    host=db_host,
    port=db_port,
    user=db_user,
    password=db_password,
    database=db_database
)

cursor = mydb.cursor()

cursor.execute("SELECT * FROM User_Records")
rows = cursor.fetchall()
columns = [column[0] for column in cursor.description]
df_user_records = pd.DataFrame(rows, columns=columns)

cursor.execute("SELECT r.record_id, r.name AS record_name, b.name AS band_name, b.genre FROM Records r INNER JOIN Bands b ON r.band_id = b.band_id")
rows = cursor.fetchall()
columns = [column[0] for column in cursor.description]
df_record_details = pd.DataFrame(rows, columns=columns)
record_details_dict = df_record_details.set_index('record_id')[['record_name', 'band_name', 'genre']].to_dict('index')

cursor.close()
mydb.close()
     

G = nx.barabasi_albert_graph(20,3)
degree_centrality = nx.degree_centrality(G)

for user_id in G.nodes:
    connected_users = list(G.neighbors(user_id))
    target_user_records = df_user_records[df_user_records['user_id'] == user_id]['record_id'].tolist()
    connected_users_records = df_user_records[df_user_records['user_id'].isin(connected_users)]['record_id'].tolist()
    recommended_records = list(set(connected_users_records) - set(target_user_records))
    recommended_records = recommended_records[:1]

    for record_id in recommended_records:
        details = record_details_dict.get(record_id, {'record_name': 'Unknown', 'band_name': 'Unknown', 'genre': 'Unknown'})
        print(f"Recommended record for user {user_id}: Record ID: {record_id}, Record Name: {details['record_name']}, Band Name: {details['band_name']}, Genre: {details['genre']}")

(i.) Let be a series of users (user_money_rates) who have certain amounts of money and have expressed their calibrated desire to buy a series of discs. Desire is expressed with a score from 1 to 5 for each disc and each disc has a monetary cost (album_price). Use a genetic algorithm to select the discs to suggest users buy in order to maximize the overall desirability of the discs, given each user's available money. Compare the results with a random selection of disks. [genetic algorithms lecture].

users_money = 200 + np.ceil(100*np.random.rand(100))
user_money_rates=np.empty_like(np.append(users_money[0],np.random.randint(5, size=100)+1))
for i in users_money:
  user_money_rates=np.vstack([user_money_rates,np.append(i,np.random.randint(5, size=100)+1)])
user_money_rates=np.delete(user_money_rates,(0),axis=0)

album_price=np.random.randint(50, size=100)+1
np.savetxt("gdrive/My Drive/user_money_rates.csv", user_money_rates, delimiter=",")
np.savetxt("gdrive/My Drive/album_price.csv", album_price, delimiter=",")
     

user_money_rates = pd.read_csv("gdrive/My Drive/user_money_rates.csv", header=None)
album_price = pd.read_csv("gdrive/My Drive/album_price.csv", header=None)

def calculate_fitness(individual, prices, rates, money):
    cost = np.sum(individual * prices)
    value = np.sum(individual * rates)
    if cost > money:
        return 0
    else:
        return value

def generate_individual(n_discs):
    return np.random.randint(2, size=n_discs)

def tournament_selection(population, fitnesses, tournament_size=3):
    selected_indices = np.random.choice(np.arange(len(population)), size=tournament_size)
    tournament_individuals = population[selected_indices]
    tournament_fitnesses = fitnesses[selected_indices]
    winner_index = selected_indices[np.argmax(tournament_fitnesses)]
    return population[winner_index]

def crossover(parents): # One point crossover.
    crossover_point = np.random.randint(len(parents[0]))
    child1 = np.concatenate((parents[0][:crossover_point], parents[1][crossover_point:]))
    child2 = np.concatenate((parents[1][:crossover_point], parents[0][crossover_point:]))
    return child1, child2

def mutation(individual, prices, money, mutation_rate=0.01):
    for i in range(len(individual)):
        if np.random.rand() < mutation_rate:
            individual[i] = 1 - individual[i]
    # Repair function.
    while np.sum(individual * prices) > money:
        selected_discs = [i for i, x in enumerate(individual) if x == 1]
        individual[np.random.choice(selected_discs)] = 0
    return individual

def genetic_algorithm(n_individuals, n_discs, prices, rates, money, n_generations=150, elitism_size=8):
    population = np.array([generate_individual(n_discs) for _ in range(n_individuals)])
    best_fitnesses = []

    for _ in range(n_generations):
        fitnesses = np.array([calculate_fitness(individual, prices, rates, money) for individual in population])
        best_fitnesses.append(np.max(fitnesses))

        # Elitism: keep the best individuals.
        elite_indices = np.argsort(fitnesses)[-elitism_size:]
        elite_individuals = population[elite_indices]

        new_population = []
        for _ in range((n_individuals - elitism_size) // 2):
            parent1 = tournament_selection(population, fitnesses)
            parent2 = tournament_selection(population, fitnesses)
            child1, child2 = crossover([parent1, parent2])
            new_population.append(mutation(child1, album_price, user_money))
            new_population.append(mutation(child2, album_price, user_money))

        population = np.concatenate((np.array(new_population), elite_individuals))

    fitnesses = np.array([calculate_fitness(individual, prices, rates, money) for individual in population])
    best_individual = population[np.argmax(fitnesses)]
    return best_individual, best_fitnesses

def get_selected_discs(individual):
    return [i for i, x in enumerate(individual) if x == 1]
     
This greedy selection works by finding the ratio of desirability to price, and then selects discs until each user's money runs out.

def greedy_selection(n_discs, prices, rates, money):
    individual = np.zeros(n_discs, dtype=int)
    disc_indices = np.argsort(-rates/prices)
    for i in disc_indices:
        if prices[i] <= money:
            individual[i] = 1
            money -= prices[i]

    return individual
     
The genetic algorithm runs below and then we plot the best fitness from generation to generation without any comparison. In the next code block there is also the comparison using "greedy selection".

album_price = album_price.values.flatten()
for i in range(user_money_rates.shape[0]):
    user_money = user_money_rates.iloc[i, 0]
    user_rates = user_money_rates.iloc[i, 1:]
    best_individual, best_fitnesses = genetic_algorithm(100, len(album_price), album_price, user_rates, user_money, elitism_size=8)
    selected_discs = get_selected_discs(best_individual)
    print(f"User {i+1} should buy discs {selected_discs}")

    # Plot the best fitnesses
    plt.figure()
    plt.plot(best_fitnesses)
    plt.xlabel('Generation')
    plt.ylabel('Best Fitness')
    plt.title(f'User {i+1}')
    plt.show()
     

greater_counter = 0
equal_counter = 0
lesser_counter = 0

for i in range(user_money_rates.shape[0]):
    user_money = user_money_rates.iloc[i, 0]
    user_rates = user_money_rates.iloc[i, 1:]

    # Genetic algorithm.
    best_individual, best_fitnesses = genetic_algorithm(100, len(album_price), album_price, user_rates, user_money, elitism_size=8)
    ga_value = np.sum(best_individual * user_rates)
    ga_cost = np.sum(best_individual * album_price)
    ga_selected_discs = get_selected_discs(best_individual)

    # Greedy selection.
    greedy_individual = greedy_selection(len(album_price), album_price, user_rates, user_money)
    greedy_value = np.sum(greedy_individual * user_rates)
    greedy_cost = np.sum(greedy_individual * album_price)
    greedy_selected_discs = get_selected_discs(greedy_individual)

    print(f"User {i+1}:")
    print(f"  GA: value = {ga_value}, cost = {ga_cost}, discs = {ga_selected_discs}")
    print(f"  Greedy: value = {greedy_value}, cost = {greedy_cost}, discs = {greedy_selected_discs}")

    if ga_value > greedy_value:
        greater_counter += 1
    elif ga_value == greedy_value:
        equal_counter += 1
    else:
        lesser_counter += 1

print("Comparison Counts:")
print(f"GA value > Greedy value: {greater_counter} times")
print(f"GA value = Greedy value: {equal_counter} times")
print(f"GA value < Greedy value: {lesser_counter} times")
